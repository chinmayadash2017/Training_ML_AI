{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    return text\n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of words 193\n"
     ]
    }
   ],
   "source": [
    "#limited vocab used, stored in a json format\n",
    "with open('important_words.json') as file:\n",
    "    important_words = json.load(file)\n",
    "    important_words = [str(w) for w in important_words]\n",
    "print('no of words',len(important_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the individual columns of word counts to their respective words from important_words\n",
    "for word in important_words:\n",
    "   products[word] =  products['review'].apply(lambda x : x.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data indices, stored in a json format\n",
    "with open('module-4-assignment-train-idx.json') as file:\n",
    "    train_indices = json.load(file)\n",
    "train_data = products.iloc[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data indices, stored in a json format\n",
    "with open('module-4-assignment-validation-idx.json') as file:\n",
    "    validation_indices = json.load(file)\n",
    "validation_data = products.iloc[validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get feature matrix and label array\n",
    "def get_numpy_data(dataframe,features,label_name):\n",
    "    #dataframe is a input dataframe\n",
    "    #features are a list of feature\n",
    "    #string for label_name\n",
    "    \n",
    "    #prepend a 'constant feature' to features list of value '1'\n",
    "    features = ['constant'] + features\n",
    "    dataframe['constant'] = 1\n",
    "    feature_matrix = dataframe[features].as_matrix()\n",
    "    label = dataframe[label_name].as_matrix()\n",
    "    \n",
    "    #return numpy 2d feature matrix and 1d array for label\n",
    "    return feature_matrix,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building on logistic regression with no L2 penalty assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probability(features,coefficient):\n",
    "    #feature matrix is a N x D matrix\n",
    "    #coefficient is a vector of shape(D)\n",
    "    return 1 / (1 + np.exp(-np.dot(features,coefficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant):\n",
    "    #error is actual -prediction having shape (N,1)\n",
    "    #jth feature vector for calculating partial derivative w.r.t jth coefficient\n",
    "    #jth coefficient\n",
    "    #l2_penalty is regularization factor(scalar)\n",
    "    #feature_is_constant: j==0 or constant(intercept/bias feature) has not considered for l2 penalty\n",
    "    \n",
    "    derivative = np.dot(error,feature)\n",
    "    if not feature_is_constant:\n",
    "        derivative = derivative - 2*l2_penalty*coefficient\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    # feature matrix is a N x D matrix\n",
    "    #sentiment is (N,1) actual label +1 for positive sentiment,-1 for negative sentiment\n",
    "    #coefficient is a vector of shape(D)\n",
    "    #l2_penalty is aregularization factor(scalar)\n",
    "    indicator = np.array([1 if i == +1 else 0 for i in sentiment])\n",
    "    score = np.dot(feature_matrix,coefficients)\n",
    "    lp = np.sum((indicator-1)*score - np.log(1 + np.exp(-score))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    return lp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients,step_size, l2_penalty, max_iter):\n",
    "    coefficients = initial_coefficients\n",
    "    for itr in range(max_iter):\n",
    "        indicator = np.array([1 if i == +1 else 0 for i in sentiment])\n",
    "        prediction = predict_probability(feature_matrix,coefficient)\n",
    "        error = indicator - prediction\n",
    "        #update each coefficient from their derivative\n",
    "        for j in range(len(coefficients)):\n",
    "            is_intercept = (j == 0)\n",
    "            derivative= feature_derivative_with_L2(error,feature_matrix[:j],coefficient,l2_penalty,is_intercept)\n",
    "            coefficients[j] = coefficients[j] + derivative\n",
    "                # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_penalty_list = [0, 4, 10, 1e2, 1e3, 1e5]\n",
    "feature_matrix = feature_matrix_train\n",
    "sentiment = sentiment_train \n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501\n",
    "coefficients_list = [logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, \n",
    "  step_size, l2_penalty, max_iter) for l2_penalty in L2_penalty_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
